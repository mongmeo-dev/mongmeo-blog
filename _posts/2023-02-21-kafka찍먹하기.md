---
Layout: post
title: 시작하자, 카프카
subtitle: 카프카 찍먹하기
tags: [MQ, Kakfa]
---

# Kafka 찍먹하기

## Topic

토픽은 카프카에서 데이터를 구분하는 개념이다. RDB의 테이블과 유사한 개념이라고 볼 수 있다. 토픽을 사용하여 데이터를 구분하고 프로듀서와 컨슈머는 원하는 토픽에 메시지를 발행하거나 필요한 토픽을 구독할 수 있다.

토픽을 생성할 때는 토픽의 이름을 반드시 설정해야 한다. 명명 규칙은 다음과 같다.

* 빈 토픽 이름은 사용할 수 없다.
* '.'또는 '..'은 토픽 이름으로 사용할 수 없다.
* 249자 이상으로 설정할 수 없다.
* 특수문자는 '_', '-', '.'만 사용할 수 있다.
* '_'를 '.'으로 변경하거나 그 반대일 때 동일한 이름이 존재한다면 토픽의 이름으로 사용할 수 없다. 예를 들어 yesaladin.coupon 토픽이 있다면 yesaladin_coupon토픽은 생성할 수 없다.

## Partition

토픽은 데이터를 구분하는 개념이고, 프로듀서와 컨슈머는 토픽에 메시지를 발행하고 구독한다. 보다 정확하게 설명하자면 토픽의 '파티션'에 메시지를 발행하고 구독한다.

파티션은 토픽을 분할한 것이다.

<video src="../wiki_file/Partition.mov"></video>

토픽을 여러 개의 파티션으로 분할하여 사용하면 같은 시간에 단일 파티션 구성보다 더 많은 양의 데이터를 처리할 수 있다. 그렇다면, 토픽을 생성할 때 파티션을 무조건 많이 만들면 좋을까?

당연히 아니다. 파티션은 각 디렉토리에 매칭되고, 저장되는 데이터마다 2개의 파일(실제 데이터와 인덱스)이 생성된다. 파티션이 많아지면 동시에 관리해야할 파일이 많아지고, 이 파일들에 대해 핸들을 열기 때문에 리소스 낭비가 심해지게 된다.

또, 파티션 리더 혹은 컨트롤러에 관련된 문제가 생길 수 있는데, 이는 추후 자세히 알아본다.

카프카에서는 토픽의 파티션을 늘리는 기능은 제공하지만 줄이는 기능은 제공하지 않는다. 파티션을 과하게 생성하여 줄이고자 한다면 토픽을 삭제했다가 재생성하는 방법밖에 없다.

그러므로 일단 적은 수의 파티션으로 운영하다가 성능 이슈가 발생하게 되면 파티션의 수를 늘리는 것을 추천한다.

## Replication Factor

카프카에서 토픽을 생성할 때 리플리케이션 팩터를 지정할 수 있다. 이는 고가용성<sub>[1](#고가용성)</sub>을 보장하기 위한 기능이다. 
한 브로커에 장애가 발생하더라도 복제된 데이터를 가지고 있는 다른 브로커가 장애가 발생한 브로커를 대체할 수 있다.

리플리케이션은 토픽 단위로 일어나지 않고 파티션 단위로 일어난다. 하지만 이해를 돕기 위해 파티션이 아닌 토픽이 복제되는 것으로 생각해보자.

브로커 3대가 하나의 클러스터로 구성된 시스템에서 토픽A의 리플리케이션 팩터가 3이라고 하면 모든 브로커에 토픽A와 그 데이터가 존재하게 된다.

3개 중 하나에는 원본 데이터가 저장되고, 나머지 두 개의 브로커에는 해당 데이터의 복제본이 저장된다. 원본 데이터가 저장되는 곳을 **리더**, 복제본이 저장되는 곳을 **팔로워**<sub>[2](#리더팔로워)</sub>라고 한다.

만약 운영 중 팔로워가 있는 브로커에 장애가 발생하게 되면 리더에는 아무런 영항이 없으므로 정상적으로 운영된다. 하지만 리더가 있는 브로커에 장애가 발생한다면 어떻게 될까?

팔로워 중 하나가 리더가 되어 요청을 정상적으로 처리한다. 클라이언트(프로듀서, 컨슈머) 입장에서는 어떤 브로커에 장애가 발생했는지, 어떤 브로커가 리더를 가지는지는 중요하지 않다. 그냥 끊김없이 데이터를 전송할 수 있으면 된다.

한 가지 알아둬야 할 것은 리더에서만 읽기, 쓰기가 일어난다는 점이다. 팔로워에서는 읽기, 쓰기가 일어나지 않고 리더의 데이터를 복제하기만 한다.

## In Sync Replica(ISR)

클라이언트와 통신하면서 읽기, 쓰기를 하는 것은 리더이고, 팔로워는 리더를 보고 있다가 데이터가 들어오면 복제해간다. 또, 리더가 다운되면 팔로워 중 하나가 리더로 승격된다.

그런데 만약 팔로워 중 하나가 장애가 발생하여 데이터를 제대로 복제하지 못한 상황에서 리더가 다운되어 팔로워 중 하나가 리더로 승격되어야 하는 일이 발생한다면 어떻게 될까?

만약 장애가 발생한 팔로워가 리더로 승격된다면 데이터 정합성을 보장할 수 없을 것이다. 이를 막기 위해서 카프카에서는 ISR(In Sync Replica)를 도입했다.

ISR을 짧게 설명하면 현재 리더를 복제하고 있는 리플리케이션 그룹이다. 리더는 팔로워들이 데이터를 제대로 복제하고 있는지 확인하다가 일정 시간동안 데이터를 복제해가지 않으면 해당 팔로워에게 장애가 발생했다고 판단하여 해당 팔로워를 ISR에서 추방한다. 이렇게 ISR에는 리더를 정상적으로 복제하고 있는 팔로워만 유지된다.

이런 상황에서 리더가 다운되어 새로운 리더가 필요하게 되면 ISR에 속해있는 팔로워 중 하나가 리더가 된다.

즉, ISR에 포함된 팔로워만이 리더가 될 수 있다. 카프카는 이렇게 ISR을 통해 리플리케이션의 신뢰성을 보장한다.



이 글에서는 토픽과 리플리케이션에 대해 아주 기본적인 부분만 짧게 다뤄보았다.

다음엔 프로듀서에 대해 간단히 알아보자.

---

[1] <a name="#고가용성">서버와 네트워크, 프로그램 등의 정보 시스템이 상당히 오랜 기간 동안 지속적으로 정상 운영이 가능한 성질</a>

[2] <a name="#리더팔로워">RabbitMQ에서는 Master Queue, Mirroed Queue라고 부른다.</a>
